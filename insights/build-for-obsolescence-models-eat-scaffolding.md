---
title: "Build for obsolescence — improving models will eat your scaffolding"
description: "Every elaborate workaround you build today becomes unnecessary as models improve; design systems that gracefully shed complexity"
topics: [ai-native-product-architecture, ai-agents]
source: "@nicbstme (Nicolas Bustamante) — Building AI Agents for Financial Services"
date: 2026-02-24
---

Everything Bustamante told his audience about skills, sandboxes, and evaluation systems? He says it's temporary. Models are improving so fast that elaborate workarounds become unnecessary. The practical advice: build for obsolescence and delete scaffolding when it becomes unnecessary.

This is especially relevant for [[skills-as-markdown-replace-fine-tuning]] — today's skill files may themselves become unnecessary as models internalize domain knowledge. The key design principle is building systems that gracefully shed complexity rather than accumulating it.

The [[ai-self-improvement-loop-accelerates-everything]] Shumer describes amplifies this urgency: GPT-5.3-Codex was instrumental in creating itself, each generation helps build the next, faster and smarter. Clara Shih at Salesforce adds a nuance — a game-changing new LLM drops every few months, so being locked to one provider is irresponsible. The systems that survive are those built on [[context-is-the-product-not-the-model]], where the value is in the context layer, not the model layer.
